{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efph_5xUbW-t"
      },
      "source": [
        "# Evaluating keyword extraction\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the evaluation of keyword extraction, the process involved using the FEVER dataset and its associated URLs. The first step was to retrieve a wiki page using the provided URL from the FEVER dataset and consider it as the ground truth. Then, keywords were extracted from the claim, and for each keyword, a wiki page was retrieved. If any of the retrieved wiki pages matched the ground truth wiki page, the extracted keyword was considered correct. This evaluation was performed on 1000 claims.\n",
        "\n",
        "To ensure data cleanliness, rows with 'evidence_sentence_id' equal to -1 were removed, indicating that the FEVER URL or wiki page contained no relevant sentences. Furthermore, we removed rows containing FEVER URLs that do not lead to a wiki page. This could be because the wiki page no longer exists or an execution error.\n",
        "\n",
        "The initial attempt involved using named entity recognition, which achieved an accuracy of 77.6%. The second attempt utilized LLM (Language Model) and instructed it to extract one or two entity names or phrases from the claim, resulting in a 71.2% accuracy.\n",
        "\n",
        "Although named entity recognition had higher accuracy, there were cases where it did not return any keywords, whereas the LLM approach consistently provided keywords, albeit with lower accuracy. For example, for the claim \"License to Wed is a movie,\" named entity recognition returned nothing [], while LLM returned ['License to Wed'], which was the correct keyword.\n",
        "\n",
        "To address this, both approaches were combined, and keywords were extracted using both methods. The extracted keywords were then aggregated, resulting in an accuracy of 87.7%.\n",
        "\n",
        "It's important to note that the ground truth was based on the FEVER URL, and even if the retrieved wiki pages did not match the ground truth, it does not necessarily mean that the retrieved pages donâ€™t contain relevant information related to the claim.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i-hkXuLWXD2o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM6Oy8SMaAt1"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb tqdm fireworks-ai python-dotenv pandas wikipedia\n",
        "!pip install sentence-transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUve9mHPZ-cQ"
      },
      "outputs": [],
      "source": [
        "import fireworks.client\n",
        "import os\n",
        "import dotenv\n",
        "import chromadb\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import wikipedia\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# you can set envs using Colab secrets\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "fireworks.client.api_key = ''\n",
        "\n",
        "def get_completion(prompt, model=None, max_tokens=50):\n",
        "    fw_model_dir = \"accounts/fireworks/models/\"\n",
        "    if model is None:\n",
        "        model = fw_model_dir + \"llama-v2-7b\"\n",
        "    else:\n",
        "        model = fw_model_dir + model\n",
        "    completion = fireworks.client.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0\n",
        "    )\n",
        "    return completion.choices[0].text\n",
        "\n",
        "mistral_llm = \"mistral-7b-instruct-4k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "mwNQQ8zgajn8",
        "outputId": "364f6207-87c9-433b-ff9f-6fe5ba240701"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Dear John Doe,\\n\\nWe, Tom and Mary, would like to extend our heartfelt gratitude for your attendance at our wedding. It was a pleasure to have you there, and we truly appreciate the effort you made to be a part of our special day.\\n\\nWe were thrilled to learn about your fun fact - climbing Mount Everest is an incredible accomplishment! We hope you had a safe and memorable journey.\\n\\nThank you again for joining us on this special occasion. We hope to stay in touch and catch up on all the amazing things you've been up to.\\n\\nWith love,\\n\\nTom and Mary\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Test LLM Api\n",
        "\n",
        "mistral_llm = \"mistral-7b-instruct-4k\"\n",
        "\n",
        "prompt = \"\"\"[INST]\n",
        "Given the following wedding guest data, write a very short 3-sentences thank you letter:\n",
        "\n",
        "{\n",
        "  \"name\": \"John Doe\",\n",
        "  \"relationship\": \"Bride's cousin\",\n",
        "  \"hometown\": \"New York, NY\",\n",
        "  \"fun_fact\": \"Climbed Mount Everest in 2020\",\n",
        "  \"attending_with\": \"Sophia Smith\",\n",
        "  \"bride_groom_name\": \"Tom and Mary\"\n",
        "}\n",
        "\n",
        "Use only the data provided in the JSON object above.\n",
        "\n",
        "The senders of the letter is the bride and groom, Tom and Mary.\n",
        "[/INST]\"\"\"\n",
        "\n",
        "get_completion(prompt, model=mistral_llm, max_tokens=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2SThXkDbdmb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"fever\", \"v1.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89KZKkFQb0K_"
      },
      "outputs": [],
      "source": [
        "X_train = dataset[\"train\"]\n",
        "X_valid = dataset[\"labelled_dev\"]\n",
        "X_test = dataset[\"paper_test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rppjHpZ2tvxu"
      },
      "outputs": [],
      "source": [
        "\n",
        "#get rid of rows that don't have sentences relevant to the claim\n",
        "filtered_data =[row for row in X_train if row['evidence_sentence_id']!=-1 ]\n",
        "data = pd.DataFrame(filtered_data)\n",
        "data=data[:2800]\n",
        "data=data.loc[:,['claim','evidence_wiki_url']]\n",
        "data=data.drop_duplicates()\n",
        "data = data.to_dict(orient='records')\n",
        "#get rid of rows which the url doesn't lead to a wiki page, either because the page no longer exist or error\n",
        "data =[row for row in data if wikipedia.search(row['evidence_wiki_url'])!=[]]\n",
        "data=data[:1000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7WGX7Wo7394",
        "outputId": "7857c158-8d0f-4781-d651-8a0394a285d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted correct keyword: 631/1000\n",
            "Did not extract correct keyword: 369/1000\n",
            "Accuracy:  0.631\n"
          ]
        }
      ],
      "source": [
        "#LLM only\n",
        "\n",
        "mistral_llm = \"mistral-7b-instruct-4k\"\n",
        "\n",
        "\n",
        "def prompt_key(claim):\n",
        "    prompt_key = f\"\"\"[INST]\n",
        "    Given the following claim, extract a query phrase to search on wikipedia for most relevant information:\n",
        "\n",
        "    CLAIM: {claim}\n",
        "\n",
        "    Extract ONE entity name or ONE phrase that when used to search on wikipedia, returns the most informative page on the claim.\n",
        "    Return ONLY the name or the phrase. DO NOT return anything else.\n",
        "    [/INST]\"\"\"\n",
        "    return prompt_key\n",
        "\n",
        "\n",
        "\n",
        "count=0\n",
        "n=1000\n",
        "for i in range(n):\n",
        "    claim = data[i][\"claim\"]\n",
        "    #get keywords\n",
        "    keyword=get_completion(prompt_key(claim), model=mistral_llm, max_tokens=150)\n",
        "    #some times the keywords returned from the LLM has space infront of its answer or in quotation marks so we have to get rid of then\n",
        "    keyword=keyword.strip().replace('\"', '').split(', ')\n",
        "    for word in keyword:\n",
        "        #some times the keyword doesnt return any wiki page, skip it so it doesn't cause a error\n",
        "        page_titles_1 = wikipedia.search(word)\n",
        "        if page_titles_1==[]:\n",
        "            continue\n",
        "        top_title_1 = page_titles_1[0]\n",
        "        page_titles_2 = wikipedia.search(data[i]['evidence_wiki_url'])\n",
        "        top_title_2 = page_titles_2[0]\n",
        "        #check if the returned wiki page match\n",
        "        if top_title_1==top_title_2:\n",
        "            count=count+1\n",
        "            #if we already found a keyword that matches the ground truth then move on to the next keyword\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'Extracted correct keyword: {count}/{n}')\n",
        "print(f'Did not extract correct keyword: {n-count}/{n}')\n",
        "print('Accuracy: ',(count)/n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU8NMZ49OBLb",
        "outputId": "398f3020-7a1c-4336-8b55-564eee2c657c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted correct keyword: 773/1000\n",
            "Did not extract correct keyword: 227/1000\n",
            "Accuracy:  0.773\n"
          ]
        }
      ],
      "source": [
        "#name entity recognition only\n",
        "def claim_extract_NER(claim):\n",
        "    doc = nlp(claim)\n",
        "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "    #it returns a entity and a entity class, we only want the entity\n",
        "    keyword = [entity[0] for entity in entities]\n",
        "    return keyword\n",
        "\n",
        "count=0\n",
        "n=1000\n",
        "for i in range(n):\n",
        "    claim = data[i][\"claim\"]\n",
        "    keywords=claim_extract_NER(claim)\n",
        "    for word in keywords:\n",
        "        page_titles_1 = wikipedia.search(word)\n",
        "        top_title_1 = page_titles_1[0]\n",
        "        page_titles_2 = wikipedia.search(data[i]['evidence_wiki_url'])\n",
        "        top_title_2 = page_titles_2[0]\n",
        "        if top_title_1==top_title_2:\n",
        "            count=count+1\n",
        "            #if we already found a keyword that matches the ground truth then move on to the next keyword\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'Extracted correct keyword: {count}/{n}')\n",
        "print(f'Did not extract correct keyword: {n-count}/{n}')\n",
        "print('Accuracy: ',(count)/n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfqpQEEY4_Ni",
        "outputId": "9f9f8c24-1663-4e4e-90a7-966410f92aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted correct keyword: 872/1000\n",
            "Did not extract correct keyword: 128/1000\n",
            "Accuracy:  0.872\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# name entity recognition and LLM\n",
        "def claim_extract(claim):\n",
        "    prompt_key = f\"\"\"[INST]\n",
        "    Given the following claim, extract a query phrase to search on wikipedia for most relevant information:\n",
        "\n",
        "    CLAIM: {claim}\n",
        "\n",
        "    Extract ONE entity name or ONE phrase that when used to search on wikipedia, returns the most informative page on the claim.\n",
        "    Return ONLY the name or the phrase. DO NOT return anything else.\n",
        "    [/INST]\"\"\"\n",
        "\n",
        "    doc = nlp(claim)\n",
        "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "    keyword_1 = [entity[0] for entity in entities]\n",
        "    keyword_2=get_completion(prompt_key, model=mistral_llm, max_tokens=150)\n",
        "    keyword_2=keyword_2.strip().replace('\"', '').split(', ')\n",
        "    keyword=keyword_1+keyword_2\n",
        "    keyword=list(set(keyword))\n",
        "    return keyword\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count=0\n",
        "n=1000\n",
        "for i in range(n):\n",
        "    claim = data[i][\"claim\"]\n",
        "    keyword=claim_extract(claim)\n",
        "    for word in keyword:\n",
        "        page_titles_1 = wikipedia.search(word)\n",
        "        if page_titles_1==[]:\n",
        "            continue\n",
        "        top_title_1 = page_titles_1[0]\n",
        "        page_titles_2 = wikipedia.search(data[i]['evidence_wiki_url'])\n",
        "        top_title_2 = page_titles_2[0]\n",
        "        if top_title_1==top_title_2:\n",
        "            count=count+1\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'Extracted correct keyword: {count}/{n}')\n",
        "print(f'Did not extract correct keyword: {n-count}/{n}')\n",
        "print('Accuracy: ',(count)/n)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}